{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='./data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# check cuda and device\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(*layers)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.gamma * self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormChannels(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, -1)\n",
    "        x = self.norm(x)\n",
    "        x = x.transpose(-1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, head_channels, shape):\n",
    "        super().__init__()\n",
    "        self.heads = out_channels // head_channels\n",
    "        self.head_channels = head_channels\n",
    "        self.scale = head_channels**-0.5\n",
    "        \n",
    "        # Using Conv2d layers instead of Linear Layers.\n",
    "        self.to_keys = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.to_queries = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.to_values = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.unifyheads = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        \n",
    "        height, width = shape\n",
    "        self.pos_enc = nn.Parameter(torch.Tensor(self.heads, (2 * height - 1) * (2 * width - 1)))\n",
    "        self.register_buffer(\"relative_indices\", self.get_indices(height, width))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        \n",
    "        keys = self.to_keys(x).view(b, self.heads, self.head_channels, -1)\n",
    "        values = self.to_values(x).view(b, self.heads, self.head_channels, -1)\n",
    "        queries = self.to_queries(x).view(b, self.heads, self.head_channels, -1)\n",
    "        \n",
    "        att = keys.transpose(-2, -1) @ queries\n",
    "        \n",
    "        indices = self.relative_indices.expand(self.heads, -1)\n",
    "        rel_pos_enc = self.pos_enc.gather(-1, indices)\n",
    "        rel_pos_enc = rel_pos_enc.unflatten(-1, (h * w, h * w))\n",
    "        \n",
    "        att = att * self.scale + rel_pos_enc\n",
    "        att = F.softmax(att, dim=-2)\n",
    "        \n",
    "        out = values @ att\n",
    "        out = out.view(b, -1, h, w)\n",
    "        out = self.unifyheads(out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_indices(h, w):\n",
    "        y = torch.arange(h, dtype=torch.long)\n",
    "        x = torch.arange(w, dtype=torch.long)\n",
    "        \n",
    "        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x, indexing='ij')\n",
    "        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1\n",
    "        indices = indices.flatten()\n",
    "        \n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.stack.imgur.com/8Mbig.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, mult=4):\n",
    "        hidden_channels = in_channels * mult\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, hidden_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_channels, out_channels, 1)   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Sequential):\n",
    "    def __init__(self, channels, head_channels, shape, p_drop=0.):\n",
    "        super().__init__(\n",
    "            Residual(\n",
    "                LayerNormChannels(channels),\n",
    "                SelfAttention2d(channels, channels, head_channels, shape),\n",
    "                nn.Dropout(p_drop)\n",
    "            ),\n",
    "            Residual(\n",
    "                LayerNormChannels(channels),\n",
    "                FeedForward(channels, channels),\n",
    "                nn.Dropout(p_drop)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStack(nn.Sequential):\n",
    "    def __init__(self, num_blocks, channels, head_channels, shape, p_drop=0.):\n",
    "        layers = [TransformerBlock(channels, head_channels, shape, p_drop) for _ in range(num_blocks)]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToPatches(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels, patch_size, hidden_channels=32):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_channels, channels, patch_size, stride=patch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionEmbedding(nn.Module):\n",
    "    def __init__(self, channels, shape):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.Tensor(channels, *shape))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToEmbedding(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels, patch_size, shape, p_drop=0.):\n",
    "        super().__init__(\n",
    "            ToPatches(in_channels, channels, patch_size),\n",
    "            AddPositionEmbedding(channels, shape),\n",
    "            nn.Dropout(p_drop)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes, p_drop=0.):\n",
    "        super().__init__(\n",
    "            LayerNormChannels(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(in_channels, classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, classes, image_size, channels, head_channels, num_blocks, patch_size,\n",
    "                 in_channels=3, emb_p_drop=0., trans_p_drop=0., head_p_drop=0.):\n",
    "        reduced_size = image_size // patch_size\n",
    "        shape = (reduced_size, reduced_size)\n",
    "        super().__init__(\n",
    "            ToEmbedding(in_channels, channels, patch_size, shape, emb_p_drop),\n",
    "            TransformerStack(num_blocks, channels, head_channels, shape, trans_p_drop),\n",
    "            Head(channels, classes, head_p_drop)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, AddPositionEmbedding):\n",
    "                nn.init.normal_(m.pos_embedding, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, SelfAttention2d):\n",
    "                nn.init.normal_(m.pos_enc, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, Residual):\n",
    "                nn.init.zeros_(m.gamma)\n",
    "    \n",
    "    def separate_parameters(self):\n",
    "        parameters_decay = set()\n",
    "        parameters_no_decay = set()\n",
    "        modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
    "        modules_no_weight_decay = (nn.LayerNorm,)\n",
    "\n",
    "        for m_name, m in self.named_modules():\n",
    "            for param_name, param in m.named_parameters():\n",
    "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "                if isinstance(m, modules_no_weight_decay):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif param_name.endswith(\"bias\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, SelfAttention2d) and param_name.endswith(\"pos_enc\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, modules_weight_decay):\n",
    "                    parameters_decay.add(full_param_name)\n",
    "\n",
    "        # sanity check\n",
    "        # assert len(parameters_decay & parameters_no_decay) == 0\n",
    "        # assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
    "\n",
    "        return parameters_decay, parameters_no_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES, IMAGE_SIZE = 10, 32\n",
    "model = ViT(NUM_CLASSES, IMAGE_SIZE, channels=32, head_channels=8, num_blocks=4, patch_size=2,\n",
    "               emb_p_drop=0., trans_p_drop=0., head_p_drop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (0): ToEmbedding(\n",
       "    (0): ToPatches(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): GELU(approximate=none)\n",
       "      (2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (1): AddPositionEmbedding()\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (1): TransformerStack(\n",
       "    (0): TransformerBlock(\n",
       "      (0): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SelfAttention2d(\n",
       "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (0): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SelfAttention2d(\n",
       "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (0): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SelfAttention2d(\n",
       "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (0): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SelfAttention2d(\n",
       "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (residual): Sequential(\n",
       "          (0): LayerNormChannels(\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): Head(\n",
       "    (0): LayerNormChannels(\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): GELU(approximate=none)\n",
       "    (2): AdaptiveAvgPool2d(output_size=1)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "    (5): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 79,810\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-1\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT: Epoch: 0 | Train Acc: 0.2085, Test Acc: 0.3552, Time: 52.5, lr: 0.001000\n",
      "ViT: Epoch: 1 | Train Acc: 0.3565, Test Acc: 0.4316, Time: 47.8, lr: 0.002000\n",
      "ViT: Epoch: 2 | Train Acc: 0.4455, Test Acc: 0.5453, Time: 47.8, lr: 0.003000\n",
      "ViT: Epoch: 3 | Train Acc: 0.4935, Test Acc: 0.5762, Time: 47.8, lr: 0.004000\n",
      "ViT: Epoch: 4 | Train Acc: 0.5234, Test Acc: 0.5755, Time: 48.2, lr: 0.005000\n",
      "ViT: Epoch: 5 | Train Acc: 0.5378, Test Acc: 0.5680, Time: 47.8, lr: 0.006000\n",
      "ViT: Epoch: 6 | Train Acc: 0.5568, Test Acc: 0.6068, Time: 48.0, lr: 0.007000\n",
      "ViT: Epoch: 7 | Train Acc: 0.5718, Test Acc: 0.5898, Time: 47.8, lr: 0.008000\n",
      "ViT: Epoch: 8 | Train Acc: 0.5802, Test Acc: 0.6042, Time: 48.0, lr: 0.009000\n",
      "ViT: Epoch: 9 | Train Acc: 0.5922, Test Acc: 0.5934, Time: 48.3, lr: 0.010000\n",
      "ViT: Epoch: 10 | Train Acc: 0.6045, Test Acc: 0.6473, Time: 48.8, lr: 0.009050\n",
      "ViT: Epoch: 11 | Train Acc: 0.6288, Test Acc: 0.6906, Time: 48.9, lr: 0.008100\n",
      "ViT: Epoch: 12 | Train Acc: 0.6436, Test Acc: 0.6783, Time: 48.4, lr: 0.007150\n",
      "ViT: Epoch: 13 | Train Acc: 0.6607, Test Acc: 0.7067, Time: 48.8, lr: 0.006200\n",
      "ViT: Epoch: 14 | Train Acc: 0.6800, Test Acc: 0.7113, Time: 48.5, lr: 0.005250\n",
      "ViT: Epoch: 15 | Train Acc: 0.6938, Test Acc: 0.7297, Time: 49.3, lr: 0.004300\n",
      "ViT: Epoch: 16 | Train Acc: 0.7072, Test Acc: 0.7354, Time: 49.1, lr: 0.003350\n",
      "ViT: Epoch: 17 | Train Acc: 0.7233, Test Acc: 0.7533, Time: 47.5, lr: 0.002400\n",
      "ViT: Epoch: 18 | Train Acc: 0.7335, Test Acc: 0.7644, Time: 49.4, lr: 0.001450\n",
      "ViT: Epoch: 19 | Train Acc: 0.7495, Test Acc: 0.7736, Time: 48.5, lr: 0.000500\n",
      "ViT: Epoch: 20 | Train Acc: 0.7601, Test Acc: 0.7804, Time: 48.2, lr: 0.000400\n",
      "ViT: Epoch: 21 | Train Acc: 0.7604, Test Acc: 0.7778, Time: 48.1, lr: 0.000300\n",
      "ViT: Epoch: 22 | Train Acc: 0.7630, Test Acc: 0.7788, Time: 48.1, lr: 0.000200\n",
      "ViT: Epoch: 23 | Train Acc: 0.7647, Test Acc: 0.7794, Time: 49.3, lr: 0.000100\n",
      "ViT: Epoch: 24 | Train Acc: 0.7658, Test Acc: 0.7810, Time: 49.3, lr: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "clip_norm = True\n",
    "lr_schedule = lambda t: np.interp([t], [0, EPOCHS*2//5, EPOCHS*4//5, EPOCHS], \n",
    "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
    "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc, n = 0, 0, 0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        model.train()\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "\n",
    "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
    "        opt.param_groups[0].update(lr=lr)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if clip_norm:\n",
    "            scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item() * y.size(0)\n",
    "        train_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    test_acc, m = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(X)\n",
    "            test_acc += (output.max(1)[1] == y).sum().item()\n",
    "            m += y.size(0)\n",
    "\n",
    "    print(f'ViT: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
